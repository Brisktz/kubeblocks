apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config-template
  namespace: {{ .Release.Namespace | quote }}
  labels:
  {{- include "llm.labels" . | nindent 4 }}

data:
  config.properties: |-
    inference_address=http://0.0.0.0:8080
    management_address=http://0.0.0.0:8081
    NUM_WORKERS=1
    number_of_gpu=1
    number_of_netty_threads=32
    job_queue_size=1000
    model_store=/data/model-store
    model_snapshot={"name":"startup.cfg","modelCount":1,"models":{"KB_LLM_MODEL_NAME_PLACEHOLDER":{"1.0":{"defaultVersion":true,"marName":"KB_LLM_MODEL_NAME_PLACEHOLDER.mar","minWorkers":1,"maxWorkers":3,"batchSize":3,"maxBatchDelay":100,"responseTimeout":120}}}}
    install_py_dep_per_model=true

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-ts-handler-config-baichuan
  namespace: {{ .Release.Namespace | quote }}
  labels:
  {{- include "llm.labels" . | nindent 4 }}

data:
  model_kwargs.py: |
    import torch
    kwargs = {
      "torch_dtype": torch.float16,
      # "low_cpu_mem_usage": True,
      "load_in_8bit": True,
      "device_map": "auto",
      "trust_remote_code": True
    }
  tokenizer_kwargs.py: |
    kwargs = {
      "use_fast": False,
      "trust_remote_code": True
    }
  requests_adapter.py: |
    import ast
    for idx, data in enumerate(requests):
      input_text = data.get("data")
      if input_text is None:
        input_text = data.get("body")
      if isinstance(input_text, (bytes, bytearray)):
        input_text = input_text.decode("utf-8")
      input_text_target = ast.literal_eval(input_text)
      messages = []
      print(input_text_target)
      messages.append(input_text_target)
      responses = []
      response = model.chat(tokenizer, messages)
      print(response)
      responses.append(response)
